{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pandas import Series , DataFrame\n",
    "import pandas as pd , numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of data values 49809663\n"
     ]
    }
   ],
   "source": [
    "data = DataFrame(pd.read_csv('C://users/grlurton/documents/dhis_kenya/data_kenya.csv'))\n",
    "data_categories = DataFrame(pd.read_csv('C://users/grlurton/documents/dhis_kenya/data_categories.csv'))\n",
    "data_elements = DataFrame(pd.read_csv('C://users/grlurton/documents/dhis_kenya/data_elements.csv'))\n",
    "org_units = DataFrame(pd.read_csv('C://users/grlurton/documents/dhis_kenya/org_units.csv'))\n",
    "org_units_groups = DataFrame(pd.read_csv('C://users/grlurton/documents/dhis_kenya/org_units_groups.csv'))\n",
    "\n",
    "# reports that organization units are supposed to make\n",
    "org_units_report = DataFrame(pd.read_csv('C://users/grlurton/documents/dhis_kenya/org_units_data_sets.csv'))\n",
    "print 'N of data values' , len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, at this stage, we only select data from reports of form MOH. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data keep : 794 indicators ; data hiv : 154 indicators\n",
      "weekly data : 29 indicators\n"
     ]
    }
   ],
   "source": [
    "reports_activity = ['oEfGJAQaARj' , 'GVGt4o7VkHk' , 'WtgleFiaJqO' , 'KbrFbt3w4bp', \n",
    "                    'b8k5cUrvVqr' , 'sfiNZ7atsK8','NLhjWzDVx4G','ovtKPo15xAg' , 'TXmr8u5rabj', 'DWKwKC6dnoB']\n",
    "\n",
    "reports_conflicting = ['lPykYxJeLKM' , 'cvyzaRp8OlE'] #These reports have the same data elements so hard to make completeness vs 0\n",
    "\n",
    "reports_hiv = ['kAofV66isvC' , 'yrYwif6R6sH' , 'GGgrU5QkjVs' , 'NJHaY8wlURg' , 'UeBJcYEoHeA' , 'IH4pYzRqTSE']\n",
    "\n",
    "data_elements_list = data_elements[data_elements.datasets_ID.isin(reports_activity)]\n",
    "data_elements_hiv = data_elements[data_elements.datasets_ID.isin(reports_hiv)]\n",
    "data_weekly = data_elements[data_elements.datasets_ID == 'zJrL4HQqYmD']\n",
    "print 'data keep :' , len(data_elements_list) , 'indicators ; data hiv :' , len(data_elements_hiv) , 'indicators'\n",
    "print 'weekly data :' , len(data_weekly), 'indicators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20710483\n",
      "141343\n"
     ]
    }
   ],
   "source": [
    "data_moh = data[data.data_element_ID.isin(data_elements_list.data_element_ID)]\n",
    "data_moh = data_moh[(data_moh['period'] != '2014AprilS1')]\n",
    "data_moh = data_moh.drop(['Unnamed: 0' , 'last_update'] , axis = 1)\n",
    "data_moh = data_moh[data_moh.data_element_ID != 'uxYaacfgY22'] # we don't care about the date the report was received\n",
    "\n",
    "\n",
    "data_weekly = data[data.data_element_ID.isin(data_weekly.data_element_ID)]\n",
    "data_weekly = data_weekly.drop(['Unnamed: 0' , 'last_update'] , axis = 1)\n",
    "\n",
    "print len(data_moh)\n",
    "print len(data_weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make a full dataset that includes explicitely all missing values. To do this we first put the data in wide format to create all the NA values. We then collapse select only reports due by each facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'oEfGJAQaARj'\n",
    "reports_activity = ['GVGt4o7VkHk' , 'WtgleFiaJqO' , 'KbrFbt3w4bp', #'b8k5cUrvVqr' , \n",
    "                    'sfiNZ7atsK8','NLhjWzDVx4G','ovtKPo15xAg' , 'TXmr8u5rabj', 'DWKwKC6dnoB']\n",
    "\n",
    "## On va voir ce qui dans 'b8k5cUrvVqr' \n",
    "\n",
    "moh_elements = data_elements[data_elements.datasets_ID.isin(reports_activity)]\n",
    "\n",
    "data_moh_merged = pd.merge(data_moh, moh_elements, left_on='data_element_ID', right_on='data_element_ID', \n",
    "                                     how='inner')\n",
    "\n",
    "data_moh_merged.loc[((data_moh_merged.datasets_ID == 'GVGt4o7VkHk') & \n",
    "                            (data_moh_merged.category == 'lHl2rmXpHse')),'datasets_ID'] = 'oEfGJAQaARj'\n",
    "\n",
    "data_moh_merged.loc[(data_moh_merged.datasets_ID == 'oEfGJAQaARj'),'datasets_name'] = \"Outpatient summary < 5 years\"\n",
    "\n",
    "data_moh_merged = data_moh_merged.drop(['data_element_url' , 'datasets_name' , 'data_element_name','Unnamed: 0'] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First a very wide melt to get complete missing values in time\n",
    "data_moh_wide = data_moh_merged.set_index(['datasets_ID',  \n",
    "                                      'data_element_ID','category' ,'org_unit_ID', 'period']).unstack(['org_unit_ID','period']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_moh_wide2 = data_moh_wide.reindex()\n",
    "#,'period',org_unit_ID'\n",
    "#help(data_moh_wide.reindex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_moh_wide2.sortlevel(axis=0,inplace=True,sort_remaining=True)\n",
    "data_moh_wide2.sortlevel(axis=1,inplace=True,sort_remaining=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to zeroify relevant mixing values\n",
    "\n",
    "#idx = pd.IndexSlice\n",
    "#nbr_na = []\n",
    "#ncol = []\n",
    "\n",
    "def how_much_null(data):\n",
    "    out = sum(pd.isnull(data))\n",
    "    return out\n",
    "\n",
    "def zeroify(data):\n",
    "    how_much = how_much_null(data)\n",
    "    total_length  = len(data)\n",
    "    if how_much < total_length:\n",
    "        data[pd.isnull(data)] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Faire une liste de tout mes facility / months pourlesquels j'ai au moins une value\n",
    "## Merger sur data complete pour garder juste les données de ces facilités / mois\n",
    "## fillna là dessus pour changer les données en 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = data_moh_wide2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report DWKwKC6dnoB 240\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report GVGt4o7VkHk 52\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report KbrFbt3w4bp 54\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report NLhjWzDVx4G 279\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report TXmr8u5rabj 48\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report WtgleFiaJqO 104\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report oEfGJAQaARj 47\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report ovtKPo15xAg 42\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n",
      "Report sfiNZ7atsK8 95\n",
      "     Let's Zeroify !!\n",
      "         Creating data frame\n"
     ]
    }
   ],
   "source": [
    "periods = data_moh['period'].unique()\n",
    "dataSets = data_moh_wide2.index.levels[0]\n",
    "\n",
    "for report in dataSets:\n",
    "    print 'Report' , report,  len(data_moh_wide2.loc[report])\n",
    "    moh_reindex_sub = data_moh_wide2.loc[(report,slice(None),slice(None)),(slice(None) , slice(None) , slice(None))]\n",
    "    print \"     Let's Zeroify !!\"\n",
    "    data_try = moh_reindex_sub.apply(zeroify)\n",
    "    print \"         Creating data frame\"\n",
    "    df1.loc[(report,slice(None),slice(None)),(slice(None) , slice(None) , slice(None))] = data_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.to_csv('C://users/grlurton/desktop/out_py.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp = range(0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moh_reindex2 = moh_reindex.loc[samp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wid = data_moh_wide.stack(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_full = df1\n",
    "#data_full.columns = [' '.join(col).strip() for col in data_full.columns.values]\n",
    "#data_full.columns = data_full.columns.str[6:29]\n",
    "\n",
    "# Then we stack on org_unit_ID, so we have all period for all org_unit, including empty ones\n",
    "#data_moh_full = data_moh_wide.stack(level=['org_unit_ID'] , dropna = False)\n",
    "\n",
    "#print data_moh_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_moh_wide_2 = data_moh_wide.stack(level=['org_unit_ID','data_element_ID'] , dropna = False)\n",
    "#data_moh_wide_2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_moh_wide_2.reset_index(inplace=True)\n",
    "#data_moh_wide_2.uniqueid = data_moh_wide_2[\"org_unit_ID\"].str.cat(data_moh_wide_2['data_element_ID'], sep=' ' )\n",
    "#ID = data_elements_requirement[\"org_unit_ID\"].str.cat(data_elements_requirement['data_element_ID'], sep=' ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_elements_requirement = pd.merge(org_units_report, df1 , left_on='dataset_ID', right_on='datasets_ID', \n",
    "                                     how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#col = data_moh_wide.columns\n",
    "sec_data = data_moh_wide\n",
    "#for fac in col.levels[1].tolist():\n",
    "    #if not (fac in data_elements_requirement[\"org_unit_ID\"].unique()) :\n",
    "        #sec_data = sec_data.drop(level = fac , axis = 1)\n",
    "        #print 'Dropped' , fac\n",
    "        \n",
    "        #indicators = data_elements_requirement[data_elements_requirement[\"org_unit_ID\"] == fac]\n",
    "        \n",
    "        #indicators[]\n",
    "        #print 'ah bah non !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "#for fac in data_elements_requirement[\"org_unit_ID\"].unique() :\n",
    "#    if fac in col.levels[1] :\n",
    "        #print data_moh_wide.loc[slice(None),(slice(None),fac,slice(None),slice(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_moh_wide.loc[slice(None),(slice(None),'A05h9ICwmHK','BwROYFIiXKG','SeDaCnHi3m5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = data_full.columns\n",
    "\n",
    "test2 = test[test.isin(ID)].tolist()\n",
    "\n",
    "print len(test2)\n",
    "\n",
    "#print test2\n",
    "\n",
    "data_ok = data_full[test2]\n",
    "\n",
    "print len(data_full.columns)\n",
    "\n",
    "#print data_full.head()\n",
    "\n",
    "#print len(data_ok.columns) , 'm' , len(data_full.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing missingness\n",
    "\n",
    "We first compute how much a orgunit should report on a monthly basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_requirement_orgunit(data_element_list , report_requirement):\n",
    "    \"\"\"\n",
    "    Function that takes a list of data elements and the reporting requirement of facilities and returns a merge data frame \n",
    "    representing a facility level requirement of different data elements\n",
    "    \"\"\"\n",
    "    data_elements_requirement = pd.merge(report_requirement, data_element_list, left_on='dataset_ID', right_on='datasets_ID', \n",
    "                                     how='inner')\n",
    "    data_elements_numbers = DataFrame(data_elements_requirement['org_unit_ID'].value_counts())\n",
    "    data_elements_numbers.columns = ['required_data_elements']\n",
    "    return data_elements_numbers\n",
    "\n",
    "def get_requirement_data_element(data_element_list , report_requirement):\n",
    "    \"\"\"\n",
    "    Function that takes a list of data elements and the reporting requirement of facilities and returns a merge data frame \n",
    "    representing a facility level requirement of different data elements\n",
    "    \"\"\"\n",
    "    data_elements_requirement = pd.merge(report_requirement, data_element_list, left_on='dataset_ID', right_on='datasets_ID', \n",
    "                                     how='inner')\n",
    "    data_elements_numbers = DataFrame(data_elements_requirement['data_element_ID'].value_counts())\n",
    "    data_elements_numbers.columns = ['required_data_elements']\n",
    "    return data_elements_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute how much orgunits actually report monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_availability_orgunit(data):\n",
    "    \"\"\"\n",
    "    Function that reports the number of data elements available for each orgunit by period \n",
    "    \"\"\"\n",
    "    data_availability = pd.pivot_table(data , index=['org_unit_ID','period'] , aggfunc = len)['data_element_ID']\n",
    "    return data_availability\n",
    "\n",
    "def get_availability_data_element(data):\n",
    "    \"\"\"\n",
    "    Function that reports the number of data elements available for each data_element by period \n",
    "    \"\"\"\n",
    "    data_availability = pd.pivot_table(data , index=['data_element_ID','period'] , aggfunc = len)['org_unit_ID']\n",
    "    return data_availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_completeness_orgunit(data , data_element_list , report_requirement):\n",
    "    \"\"\"\n",
    "    Function that computes the data requirement and availability, and returns a description of completeness\n",
    "    \"\"\"\n",
    "    required = get_requirement_orgunit(data_element_list , report_requirement)\n",
    "    available = get_availability_orgunit(data)\n",
    "    completeness = pd.merge(available.reset_index() , required.reset_index() , \n",
    "                            left_on = 'org_unit_ID' , right_on = 'index' ,\n",
    "                            left_index=True, \n",
    "                            how='inner', sort=False)\n",
    "    completeness['completeness'] = completeness['data_element_ID'] / completeness['required_data_elements']\n",
    "    completeness = DataFrame(completeness)\n",
    "    completeness = completeness[(completeness.completeness <= 1) & (completeness.period.astype(int) > 201012)]\n",
    "    return completeness\n",
    "\n",
    "\n",
    "def get_completeness_data_element(data , data_element_list , report_requirement):\n",
    "    \"\"\"\n",
    "    Function that computes the data requirement and availability, and returns a description of completeness\n",
    "    \"\"\"\n",
    "    required = get_requirement_data_element(data_element_list , report_requirement)\n",
    "    print required.head()\n",
    "    available = get_availability_data_element(data)\n",
    "    completeness = pd.merge(available.reset_index() , required.reset_index() , \n",
    "                            left_on = 'data_element_ID' , right_on = 'index' ,\n",
    "                            left_index=True, \n",
    "                            how='inner', sort=False)\n",
    "    completeness['completeness'] = completeness['org_unit_ID'] / completeness['required_data_elements']\n",
    "    \n",
    "    completeness = DataFrame(completeness)\n",
    "    completeness = completeness[(completeness.completeness <= 1) & (completeness.period.astype(int) > 201012)]\n",
    "    return completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Making completeness for OU'\n",
    "data_completeness_fac = get_completeness_orgunit(data_moh , data_elements_list , org_units_report)\n",
    "print 'Making completeness for DE'\n",
    "data_completeness_de = get_completeness_data_element(data_moh , data_elements_list , org_units_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data_completeness_de['required_data_elements'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_completeness_de2 = data_completeness_de[data_completeness_de[\"required_data_elements\"]  < 8970]\n",
    "print 100*len(data_completeness_de2)  / len(data_completeness_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_completeness_de2 = data_completeness_de[data_completeness_de[\"required_data_elements\"]  < 7000]\n",
    "data_completeness_de2 = data_completeness_de2.drop(['level_0' , 'org_unit_ID' , 'index' , \n",
    "                                                    'required_data_elements' ] , axis = 1)\n",
    "#data_completeness_de2.reset_index(inplace=True) \n",
    "print len(data_completeness_de2)\n",
    "print len(data_completeness_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = data_completeness_de2.set_index(['period' , 'data_element_ID']).unstack(['data_element_ID']) \n",
    "print df.plot(legend = False , figsize = [15,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many values do we have ?\n",
    "n_values_1 = len(data_moh_wide) - pd.isnull(data_moh_wide).sum(axis=0)\n",
    "n_data_points = len(data_moh_wide)\n",
    "print 'Maximum completeness for an indicator is' , (n_values_1 / n_data_points).max()\n",
    "print 'Minimum completeness for an indicator is' , (n_values_1 / n_data_points).min()\n",
    "print 'Mean completeness for an indicator is' , (n_values_1 / n_data_points).mean()\n",
    "print 'Median completeness for an indicator is' , (n_values_1 / n_data_points).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAVEAT : An unrlying hypothesis here is that every orgunit has to report on every data set every month. It is thus a raw UNDERESTIMATION of completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_moh_wide = data_moh_wide.unstack(1)\n",
    "\n",
    "n_values_2 = len(data_moh_wide) - pd.isnull(data_moh_wide).sum(axis=0)\n",
    "\n",
    "n_periods = data_moh.period.nunique()\n",
    "print 'Maximum completeness for an indicator serie for a facility is' , (n_values_2 / n_periods).max()\n",
    "print 'Minimum completeness for an indicator  for a facility is' , (n_values_2 / n_periods).min()\n",
    "print 'Mean completeness for an indicator  for a facility is' , (n_values_2 / n_periods).mean()\n",
    "print 'Median completeness for an indicator  for a facility is' , (n_values_2 / n_periods).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here, we only want to keep facilities with at least one serie with over 75% completness, and series with at least \n",
    "# 50%  overall completeness\n",
    "\n",
    "completeness_fac = DataFrame(n_values_2 / n_periods > 0.75)\n",
    "completeness_de = DataFrame(n_values_1 / n_data_points > 0.5)\n",
    "\n",
    "completeness_fac.reset_index(inplace=True) \n",
    "completeness_de.reset_index(inplace=True) \n",
    "\n",
    "de_keep = completeness_de[completeness_de[0] > 0.5].data_element_ID.unique()\n",
    "fac_keep = completeness_fac[completeness_fac[0] > 0.5].org_unit_ID.unique()\n",
    "print 'We want to keep' , 100 * len(fac_keep) / len(completeness_fac.org_unit_ID.unique()) , '% of facilities'\n",
    "print 'We want to keep' , 100 * len(de_keep) / len(completeness_de.data_element_ID.unique()) , '% of indicators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_comp = data_moh[data_moh['org_unit_ID'].isin(fac_keep) & data_moh['data_element_ID'].isin(de_keep)]\n",
    "print 'We are keeping' , 100*len(data_comp) / len(data_moh) , '% of the data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### And rebuilding the missingness\n",
    "\n",
    "# First a very wide melt to get complete missing values in time\n",
    "data_comp_wide = data_comp.set_index(['org_unit_ID', 'period', \n",
    "                                      'category','data_element_ID']).unstack(['org_unit_ID'  ,'data_element_ID',\n",
    "                                                                                 'category']) \n",
    "\n",
    "# Then we stack on org_unit_ID, so we have all period for all org_unit, including empty ones\n",
    "data_comp_wide = data_comp_wide.stack(level=['org_unit_ID'] , dropna = False)\n",
    "\n",
    "\n",
    "\n",
    "## We need to rename columns. not the simplest now\n",
    "data_comp_wide.columns = [' '.join(col).strip() for col in data_comp_wide.columns.values]\n",
    "data_comp_wide.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMELIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%R rm(list = ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R --input data_comp_wide\n",
    "\n",
    "save.image('//ihme.washington.edu/IHME/HOME/grlurton/imputation/python_export.RData')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%R --input data_comp_wide --output a_out\n",
    "library(Amelia)\n",
    "\n",
    "cols <- seq(2,ncol(data_comp_wide))\n",
    "\n",
    "data_comp_wide[,cols] = apply(data_comp_wide[,cols], 2, function(x) as.numeric(as.character(x)))\n",
    "    \n",
    "    \n",
    "a_out <- amelia(data_comp_wide , cs = \"org_unit_ID\" , ts = \"period\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ameliorer le travail R en exportant les donnees en Rdata, et reimportant les resultats.\n",
    "\n",
    "Faire tourner en parrallele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAJOUTER LES COVARIABLES SITES. => sinon on a des observations completement missing\n",
    "\n",
    "en plus faire tourner d'abord sur le subset qui m'interesse et apres sur le reste "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A venir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA / Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catarina : amelia + arima : more or less similar\n",
    "\n",
    "arima est bon pour les facilites avec donnes partielles + statifie\n",
    "\t? code \n",
    "\n",
    "amelia : everything in one go : on annual data\n",
    "\tset threshold : si facility reports < 60 : set as missing.\n",
    "\tfind most important covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Autres méthodes : MICE et Missforest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
